LANGUAGE DETECTION

DATASET SOURCE: kaggle (contains data about 22 popular languages and contains 1000 sentences in each of the languages)



LIBRARY: sklearn.feature_extraction
    -The //sklearn.feature_extraction// module can be used to extract features in a format supported by machine learning algorithms from datasets consisting of formats such as text and image.
    -Feature extraction transforms arbitrary data, such as text or images, into numerical features usable for machine learning.


TEXT FEATURE EXTRACTION USING "BAG OF WORDS / BAG OF N-GRAMS REPRESENTATION"
    Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.

    In order to address this, scikit-learn provides utilities for the most common ways to extract numerical features from text content, namely:
    --"tokenizing" strings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators.
    --"counting" the occurrences of tokens in each document.
    --"normalizing" and weighting with diminishing importance tokens that occur in the majority of samples / documents.

    In this scheme, features and samples are defined as follows:
    1)each individual token occurrence frequency (normalized or not) is treated as a "feature".
    2)the vector of all the token frequencies for a given document is considered a "multivariate sample".

    A lot of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus.
    We call "vectorization" the general process of turning a collection of text documents into numerical feature vectors.Documents are described by word occurrences while completely ignoring the relative position information of the words in the document.

    This specific strategy (tokenization, counting and normalization) is called the Bag of Words or “Bag of n-grams” representation.

SPARSITY
    As most documents will typically use a very small subset of the words used in the corpus, the resulting matrix will have many feature values that are zeros (typically more than 99% of them). Every document might have a unique word that does not repeat in any other documents, hence for that feature, values will be 0 in those documents(rows).

    In order to be able to store such a matrix in memory but also to speed up algebraic operations matrix / vector, implementations will typically use a sparse representation such as the implementations available in the //scipy.sparse// package.

COMMON VECTORIZER USAGE (//from sklearn.feature_extraction.text import CountVectorizer//
)
    CountVectorizer implements both tokenization and occurrence counting in a single class. Converts a collection of text documents to a matrix of token counts. It is used to transform a given text into a vector on the basis of the frequency (count) of each word that occurs in the entire text. This is helpful when we have multiple such texts, and we wish to convert each word in each text into vectors (for using in further text analysis). CountVectorizer creates a matrix in which each unique word is represented by a column of the matrix, and each text sample from the document is a row in the matrix. The value of each cell is nothing but the count of the word in that particular text sample. All words have been converted to lowercase. The words in columns have been arranged alphabetically. Inside CountVectorizer, these words are not stored as strings. Rather, they are given a particular index value.
    
    This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix.
    
    This model has many parameters, however the default values are quite reasonable. If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data.

    Stop Words: 
    Stop words are words like “and”, “the”, “him”, which are presumed to be uninformative in representing the content of a text, and which may be removed to avoid them being construed as signal for prediction. Sometimes, however, similar words are useful for prediction, such as in classifying writing style or personality.
    Please take care in choosing a stop word list. Popular stop word lists may include words that are highly informative to some tasks, such as 'computer'.
    You should also make sure that the stop word list has had the same preprocessing and tokenization applied as the one used in the vectorizer. 

MODEL SELECTION - TRAIN TEST SPLIT (//from sklearn.model_selection import train_test_split//)
    -Split arrays or matrices into random train and test subsets. Returns List containing train-test split of inputs.
    -test_size---float or int, default=None
    -If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. If train_size is also None, it will be set to 0.25.
    -train_size---float or int, default=None
    -If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size.
    -random_state---int, RandomState instance or None, default=None
    -Controls the shuffling applied to the data before applying the split. Pass an int for reproducible output across multiple function calls.

MULTINOMIAL NAIVE BAYES CLASSIFIER
    -Naive Bayes classifier for multinomial models. 
    -The multinomial logistic model assumes that data are case-specific; that is, each independent variable has a single value for each case. The multinomial logistic model also assumes that the dependent variable cannot be perfectly predicted from the independent variables for any case.
    -Naive Bayes classifiers are a collection of classification algorithms based on Bayes’ Theorem. Common Principle( naive conditional independence assumption): every pair of features being classified is independent of each other. Bayes’ Theorem finds the probability of an event occurring given the probability of another event that has already occurred.
    -Multinomial Naive Bayes: Feature vectors represent the frequencies with which certain events have been generated by a multinomial distribution. This is the event model typically used for document classification.
    -The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification, document classification and spam filtering). 
    -They require a small amount of training data to estimate the necessary parameters.
    -MultinomialNB implements the naive Bayes algorithm for multinomially distributed data. The distribution is parametrized by vectors.
    - fit(X, y[, sample_weight]) ---Fit Naive Bayes classifier according to X(---Training vectors---{array-like, sparse matrix}  of shape (n_samples, n_features) ), y(---Target values---array-like of shape (n_samples,)), sample_weight(Weights applied to individual samples (1. for unweighted).---array-like ---of shape (n_samples,)
    - score(X, y, sample_weight=None) --- Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.

PANDAS
    -Pandas is an open-source library that is made mainly for working with relational or labeled data both easily and intuitively. It is used to 'read' the data.
    -It provides various data structures and operations for manipulating numerical data and time series.
    -This library is built on top of the NumPy library.
    -Easy handling of missing data (represented as NaN(NaN stands for Not A Number)) in floating point as well as non-floating point data
    -pandas series
    -pandas dataframe : Pandas DataFrame is a two-dimensional size-mutable, potentially heterogeneous tabular data structure with labeled axes (rows and columns). It consists of the data, rows, and columns.
    - a Pandas DataFrame will be created by loading the datasets from existing storage, storage can be SQL Database, CSV file, an Excel file. 

NUMPY
    -NumPy is a general-purpose array-processing package.
    - It provides a high-performance multidimensional array object, and tools for working with these arrays.
    -It is the fundamental package for scientific computing with Python. -It is open-source software.
    -Useful linear algebra, Fourier transform, and random number capabilities
    -homogeneous multidimensional array:  a table of elements (usually numbers), all of the same type, indexed by a tuple of positive integers.
    -arr = np.array([[1, 2, 3, 4],
                [5, 2, 4, 2],
                [1, 2, 0, 1]])
    -NumPy offers several functions to create arrays with initial placeholder content. These minimize the necessity of growing arrays, an expensive operation. For example: np.zeros, np.ones, np.full, np.empty, etc.
    -Reshaping array: We can use reshape method to reshape an array.
    -Flatten array: We can use flatten method to get a copy of array collapsed into one dimension. It accepts order argument. Default value is ‘C’ (for row-major order). Use ‘F’ for column major order.
    - Sorting array: There is a simple np.sort method for sorting NumPy arrays. 
